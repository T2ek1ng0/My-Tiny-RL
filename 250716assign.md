状态转移概率
| 起点状态&采取的动作 | 终点状态为S<sub>1</sub> | 终点状态为S<sub>2</sub> | 终点状态为S<sub>3</sub> |
| :------------------ | ----------------- | ----------------- | ----------------- |
| $S_{1}A_{1}$        | $0.1$             | $0.4$             | $0.5$             |
| $S_{1}A_{2}$        | $0.2$             | $0.6$             | $0.2$             |
| $S_{2}A_{1}$        | $0.4$             | $0.2$             | $0.4$             |
| $S_{2}A_{2}$        | $0.5$             | $0.25$            | $0.25$            |
| $S_{3}A_{1}$        | $0.3$             | $0.7$             | $0$               |
| $S_{3}A_{2}$        | $0.3$             | $0.1$             | $0.6$             |

奖励

| 到达状态 | 奖励 |
| -------- | ---- |
| $S_1$    | $1$  |
| $S_2$    | $5$  |
| $S_3$    | $2$  |

其中
$\gamma = 0.9 , \alpha = 0.1 , \varepsilon = 0.3 , T = 100$

Sarsa算法最优动作价值函数更新规则

$Q(S_{t},A_{t})\leftarrow Q(S_{t},A_{t})+\alpha [R_{t}+\gamma Q(S_{t+1},A_{t+1})-Q(S_{t},A_{t})]$

Q-Learning算法最优动作价值函数更新规则

$Q(S_{t},A_{t})\leftarrow Q(S_{t},A_{t})+\alpha [R_{t}+\gamma max_{A_{t+1}}Q(S_{t+1},A_{t+1})-Q(S_{t},A_{t})]$

